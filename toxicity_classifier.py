# -*- coding: utf-8 -*-
"""Toxicity Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NTzsVBZwHsSY6H5xk0yDkVlkH7KjyB9Y

# Introduction
By: Lucy Hu, Sean McGeehan, James Baker

For our project we focused on classifying toxicity using syntax and semantic features. We trained several machine learning models ranging from simple Bag of Words models using tf-idf up to fine tuning a BERT base model.

## Motivation
The introduction of widespread Internet access and the age of social media has allowed for much more communication, interaction, and information sharing than ever before. Users are able to engage with far-away family, friends, and even strangers from the comfort of their own home. While social media has no doubt brought greater convenience and connection into our lives, its potential negative consequences on mental health, social abilities, and more cannot be ignored. In fact, they are the subject of much concern today, as evidenced by numerous [articles](https://www.wsj.com/articles/smarter-healthier-social-media-choices-11639177212) and [studies](https://www.pewresearch.org/fact-tank/2020/10/15/64-of-americans-say-social-media-have-a-mostly-negative-effect-on-the-way-things-are-going-in-the-u-s-today/) published on the subject. 


Unfortunately, social media can be abused as a platform for creating and sharing offensive and toxic content like hate speech. As a result, comment moderation has become increasingly important especially considering the far reaching impacts of social media. [Studies](https://www.pewresearch.org/internet/fact-sheet/social-media/) [conducted](https://www.pewresearch.org/internet/fact-sheet/social-media/) by the Pew Research Center suggest that at least 72% of the public uses some form of social media and the majority of users access these platforms on a daily basis. Even more distressing, many of the most prolific users are young adults or adolescents. Hate speech and other forms of offensive content can potentially inflict mental and physical harm on users, especially vulnerable groups. 

Given the importance of this problem, we decided to explore toxicity classification for our project.

## Data and Definition
Our dataset originates from the [Jigsaw Unintended Bias in Toxicity Classification competition](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data?select=train.csv) on Kaggle. It consists of 1.8 million comments which were gathered from various Wikipedia Talk pages and labeled by a panel of reviewers. A target toxicity label was generated by aggregating the ratings of all judges, where toxicity was defined by researchers as "anything rude, disrespectful or otherwise likely to make someone leave a discussion."

The table below includes examples of comments that were classified as non-toxic and toxic:

| Comment                                                                                                                                        | Classification |
|------------------------------------------------------------------------------------------------------------------------------------------------|----------------|
| Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!                             | Non-toxic      |
| This is such an urgent design problem; kudos to you for taking it on. Very impressive!                                                         | Non-toxic      |
| please spare us your strawman drivel that is void of intelligence. Impeding traffic causing accidents has nothing to do with being in a hurry. | Toxic          |
| For some of these demonic possessed brats, a straitjacket would be the least restrictive appropriate educational setting.                      | Toxic          |

## Hypothesis

We define the null hypothesis as a non-significant correlation between comment contents and offensive rating. Our machine learning model rejects the null hypothesis by learning a model with a set of heuristics and comment attributes as inputs and offensiveness label (True or False) as the output. As shown below, such a model exists and operates through various methods, including but not limited to Deep Learning Transformers, Logistic Regression, and Random Forest.

## Summary of Approach

We approached this task first by cleaning our data and generating a binary toxicity label before conducting some exploratory data analysis. Based on our results, we began to generate potential features such as Bag of Words (e.g. tf-idf), word embedding using Word2vec, a count of George Carlin's list of offensive words, stopwords, sentence length and count, comma count, social justice buzzwords, reading level, and more. We explored using logistic regression, xgboost, and pretrained roBERTa models as well as fine tuning our own BERT and roBERTa models.

#Import Spacy data for Word2vec
Run the following cell at the beginning of the notebook, then reset runtime and do not run again.
"""

# Spacy setup -- reset notebook after running this cell
! python -m spacy download en_core_web_lg

"""# Imports and Preparing Data

Importing Pandas and NLTK
"""

# Import basic libraries

import pandas as pd
import numpy as np

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

"""Import data"""

# Download data
!apt update
!apt install gcc python-dev libkrb5-dev
from google_drive_downloader import GoogleDriveDownloader as gdd

gdd.download_file_from_google_drive(file_id='163YCf74tmFDeN_vy64Ta_VgAZwTddqoo',
                                    dest_path='/content/train.csv')

# Importing data
df = pd.read_csv('/content/train.csv', error_bad_lines=False) 
df

"""Here we clean the imported dataframe. In order to create models that would be more applicable to other data sources (e.g. social media comments gathered from Twitter, Reddit, Youtube, etc.), we decided to focus only on toxicity and dropped the majority of other columns, which contained data which would not be as readily available from other sources (i.e. labels for obscenity, threats, commenter identities, etc.). """

# Data cleaning
df = df.dropna(axis = 1) # drop NA values
df= df.drop(columns = ['severe_toxicity', 'obscene', 'identity_attack', 
                                    'insult', 'threat', 'created_date', 'publication_id', 
                                    'article_id', 'rating', 'funny', 'wow', 'sad', 'likes', 
                                    'disagree', 'sexual_explicit', 'identity_annotator_count', 
                                    'toxicity_annotator_count'])

"""To better understand the distribution of our data, we looked at the number of values considered toxic by target thresholds of 0, 0.1, 0.25, 0.5, 0.75, and 1.0. As seen below, there are far more non-toxic comments than toxic comments in this dataset. Using a 0.5 threshold would mark only ~8% of comments as toxic."""

total = len(df)
thresholds = [0, 0.1, 0.25, 0.5, 0.75, 1]

print("total:", total)
for x in thresholds:
  if x == 0:
    count = len(df[df["target"] > 0])
  else: 
    count = len(df[df["target"] >= x])
  print(x, "threshold:", count, "ratio:", count / total)

"""Next, we generate a binary label to mark whether a comment is toxic or not. We set the threshold as 0.1 so that comments that were considered toxic by at least 10% of reviewers are marked as toxic in our dataset (roughly 30% of the comments). This gives us reasonable assurance that any comment that is remotely toxic will be considered as such in our models. It also insures that we have enough data to avoid having a high bias low variance data set with class imbalance.


"""

### Data wrangling
# Create labels based on target threshold
df["label"] = df["target"].apply(lambda x : 1 if x > 0.1 else 0)

"""Here, we down sample our data to create smaller datasets for model testing purposes and also create a 80/20 train test split. """

from sklearn.model_selection import train_test_split
df_small = df.sample(frac=.01, random_state=5)
df_train_sm, df_test_sm = train_test_split(df_small, test_size = .2)

df_medium = df.sample(frac=.1, random_state=5)
df_train_md, df_test_md = train_test_split(df_medium, test_size = .2)

df_train_lg, df_test_lg = train_test_split(df, test_size = .2)

"""# Graphs and Data Exploration"""

df_small_not_offensive = df_train_sm.loc[df_train_sm['label'] == 1]
df_small_offensive = df_train_sm.loc[df_train_sm['label'] == 0]

"""## Can we get a sense of which words commonly show up in either offensive or non offensive comments?
In this section we dove deep into the data to attempt to find lists of words that might be useful for our models. We generated bar plots and word clouds to help visualize what would be useful.

Based on the results, we may be able to compile a list of relevant words/features that can predict a negative comment with high fidelity. 
"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Not offensive word cloud
text = df_small_not_offensive['comment_text'].values 
wordcloud = WordCloud().generate(str(text))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

# Offensive word cloud
text = df_small_offensive['comment_text'].values 
wordcloud = WordCloud().generate(str(text))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

df_small_offensive['comment_text'].str.split(expand=True).stack().value_counts().head(10).plot(kind='bar')

df_small_not_offensive['comment_text'].str.split(expand=True).stack().value_counts().head(10).plot(kind='bar')

"""We notice that stopwords are increasing prevalant and add a signficant amount of noise. We lowercase all stop words and remove them based on a canonical English stopwords list."""

# Remove stop words
from nltk.corpus import stopwords
stop = stopwords.words('english')

# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.
df_small_offensive['comment_without_stopwords'] = df_small_offensive['comment_text'].apply(lambda x: ' '.join([word.lower() for word in x.split() if word.lower() not in (stop)]))
df_small_not_offensive['comment_without_stopwords'] = df_small_not_offensive['comment_text'].apply(lambda x: ' '.join([word.lower() for word in x.split() if word.lower() not in (stop)]))

df_small_offensive['comment_without_stopwords'].str.split(expand=True).stack().value_counts().head(10).plot(kind='bar')

df_small_not_offensive['comment_without_stopwords'].str.split(expand=True).stack().value_counts().head(10).plot(kind='bar')

# Find difference between word frequencies in offensive and not offensive comments 
df_small_offensive['comment_without_stopwords'].str.split(expand=True).stack().value_counts()

offensive_freq = df_small_offensive['comment_without_stopwords'].str.split(expand=True).stack().value_counts()
not_offensive_freq = df_small_not_offensive['comment_without_stopwords'].str.split(expand=True).stack().value_counts()
best_offensive_words = (offensive_freq-not_offensive_freq).fillna(offensive_freq)

# Sort so most offensive words percolate to top
best_offensive_words = best_offensive_words.sort_values()

# best_offensive_words
best_offensive_words.head(20).plot(kind='bar')

best_not_offensive_words = (not_offensive_freq-offensive_freq).fillna(not_offensive_freq)
best_not_offensive_words = best_not_offensive_words.sort_values()
best_not_offensive_words.head(20).plot(kind='bar')

"""Can we use a third party sentiment analyzer to see if our inferences/heuristics match the ground truth?"""

from textblob import TextBlob

df_small_offensive['emotion'] = df_small_offensive['comment_without_stopwords'].apply(lambda x:TextBlob(x).sentiment.polarity)
df_small_not_offensive['emotion'] = df_small_not_offensive['comment_without_stopwords'].apply(lambda x:TextBlob(x).sentiment.polarity)

# Plot distribution of emotion scores
# Seems to be somewhat normally distributed with a mode around .5
df_small_offensive.hist(column='emotion', bins=30)

# Plot distribution of emotion scores
# Seems to be right skewed with mode around .5
df_small_not_offensive.hist(column='emotion', bins=30)

# Plot emotion vs length
df_small_not_offensive['length_of_comment'] = df_small_not_offensive['comment_without_stopwords'].apply(lambda x: len(x))
df_small_offensive['length_of_comment'] = df_small_offensive['comment_without_stopwords'].apply(lambda x: len(x))

"""Are there other heuristics/features we can pull from the data to increase model accuracy such as comment length?"""

df_small_offensive.plot(x='length_of_comment', y='emotion', style='o')

df_small_not_offensive.plot(x='length_of_comment', y='emotion', style='o')

"""Comment length seem to not be a relevant factor in determined offensive. However, our dataset significantly skews towards shorter (200 characters or less) comments so some signal may be lost in the noise. Using toxic words instead seems to be a more fruitful pursuit. """

# Don't these look offensive??
df_small_not_offensive

df_small_offensive

"""# Feature Generation and Modeling

## tf-idf

Term frequency-inverse document frequency can be used to determine how important a word is in the context of a corpus of text. We decided to incorporate this statistic into our model to introduce weighting and distinguish important words (i.e. words that appear repeatedly but only in one document or comment in this case). This helps to decrease weighting of repeated words (e.g. stop words such as "you" or "the") and ideally give more weight to words that appear only in selected cases, such as the ones we saw in our exploration above (e.g. "stupid", "ignorant", etc.)
"""

# tf-idf word
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
X_tfidf_train_sm = vectorizer.fit_transform(df_train_sm["comment_text"])
X_tfidf_test_sm = vectorizer.transform(df_test_sm["comment_text"])

X_tfidf_train_md = vectorizer.fit_transform(df_train_md["comment_text"])
X_tfidf_test_md = vectorizer.transform(df_test_md["comment_text"])

X_tfidf_train_lg = vectorizer.fit_transform(df_train_lg["comment_text"])
X_tfidf_test_lg = vectorizer.transform(df_test_lg["comment_text"])

"""The dataset that we chose to explore contained many more non-toxic comments than toxic comments, which could potentially bias the results of our models (e.g. a model that always predicts non-toxicity would be right the majority of the time). To combat this, we used a lower threshold for our binary label (i.e. 0.1 instead 0.5) and also set the class weight for our model to be balanced. This means that non-toxic samples will receive lower weighting than toxic samples to account for the fact that there are fewer toxic comments.  """

# Logistic regression w/ tf-idf 
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(class_weight = 'balanced', max_iter = 400)
model.fit(X_tfidf_train_lg, df_train_lg["label"])
p = model.predict_proba(X_tfidf_test_lg)[:,1]

# AUC for logistic regression w/ tf-idf 
from sklearn.metrics import roc_auc_score
roc_auc_score(df_test_lg['label'], p)

"""We can take a look at the results of our model to get a better understanding of which words were determined to be more important. As you can see in the output below, the top 20 words include “idiot”, “fool”, “stupid”, “moron”, as well as different variations on those base words. All of these words do have negative or hurtful connotations, which supports the success of the model."""

# Reverse tf-idf vocabulary mappings
indexToWords = {value : key for key, value in vectorizer.vocabulary_.items()}
v = list(model.coef_[0])
coef = [coef for coef, id in list(zip(v, range(len(v))))]
words = [indexToWords[id] for coef, id in list(zip(v, range(len(v))))]
wordsWithCoeff = pd.DataFrame({"Coefficient" : coef, "Word" : words})
wordsWithCoeff.sort_values(by='Coefficient', ascending=False).head(20)

"""Additionally, we tried a model using xgboost on the same feature matrix in order to explore non-linear decision boundaries (seen below). However, because our dataset is so large, we could only successfully run this on our medium dataset (1/10th of the full 1.8 million comments). Based on AUC scores run on the same dataset, we determined the logistic regression appeared more successful (~0.85 as compared to ~0.74) and so we chose to continue with that model instead. """

# xgboost w/ tf-idf 
import xgboost as xgb
from xgboost import XGBClassifier

model_tfidf_xgb = XGBClassifier(objective = "binary:logistic", max_depth=5, n_estimators=100)
model_tfidf_xgb.fit(X_tfidf_train_md, df_train_md["label"])
p_tfidf_xgb = model_tfidf_xgb.predict_proba(X_tfidf_test_md)

# AUC for xgboost w/ tf-idf 
roc_auc_score(df_test_md['label'], p_tfidf_xgb[:,1])

"""Expanding beyond just a simple word tf-idf statistic, we explored including character n-grams (where n = 1, 2, and 3) in order to address cases where users misspell or attempt to modify their words by using other characters or spellings (e.g. "2night" instead of "tonight"). To do this, we create another vectorizer which analyzes based on characters with a set ngram range and concatenate the results with the word-only matrix. """

# tf-idf char
vectorizer2 = TfidfVectorizer(analyzer='char', ngram_range = (1,3))
X_tfidf_train2 = vectorizer.fit_transform(df_train_lg["comment_text"])
X_tfidf_test2 = vectorizer.transform(df_test_lg["comment_text"])

# Concatenate char w/ word 
from scipy.sparse import hstack

X_tfidf_train3 = hstack([X_tfidf_train_lg, X_tfidf_train2])
X_tfidf_test3 = hstack([X_tfidf_test_lg, X_tfidf_test2])

# Logistic regression w/ tf-idf (char and word)
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(class_weight = 'balanced', max_iter = 1000)
model.fit(X_tfidf_train3, df_train_lg["label"])
p = model.predict_proba(X_tfidf_test3)[:,1]

# AUC for logistic regression w/ TFIDF (char and word)
roc_auc_score(df_test_lg['label'], p)

"""Ultimately, for this dataset, including the character n-grams did not make a significant difference as compared to the model based on words alone. However, we believe this remains a potentially important technique if there are many instances of word manipulation to avoid detection.

## Word2vec

Early on in the project, we explored using a Word2vec feature in a model with xgboost. Instead of representing a comment with one hot encoding, Word2vec represents each sentence as a high dimensional real vector. We successfully ran the model on a smaller sample of our dataset (1/100
 of the total), but due to time complexity of spacy Word2vec and average AUC scores (~0.78) as compared to other models we ran on the same sampled dataset, we decided to focus on pursuing other models.
"""

# Import libraries and load data
import spacy
import numpy as np
nlp = spacy.load("en_core_web_lg")

# Generate vectors for training and testing
X_w2v_train = np.array([nlp(text).vector for text in df_train_sm["comment_text"]])
X_w2v_test = np.array([nlp(text).vector for text in df_test_sm["comment_text"]])

# Create binary logistic xgboost model and fit
model_w2v = XGBClassifier(objective = "binary:logistic", max_depth=10, n_estimators=100)
model_w2v.fit(X_w2v_train, df_train_sm["label"])
p_w2v = model_w2v.predict_proba(X_w2v_test)

# AUC score for Word2vec
from sklearn.metrics import roc_auc_score

roc_auc_score(df_test_sm['label'], p_w2v[:,1])

"""## Logistic Regression on Features

### Feature generation 
(Carlin words, Trump, stop words, sentence length/count, comma count, reading level, swear words)

Next we tried to generate our own features by manipulating the text to find unique properties. Our first feature we pulled was a count of the famous 7 dirty words that George Carlin claimed could not be used on television. The routine was aired on radio station WBAI and the broadcast was promptley found "indecent but not obscene" by the Supreme Court in FCC v. Pacifica Foundation




https://www.youtube.com/watch?v=kyBH5oNQOS0
"""

# Determine count of Carlin words
carlin_words = set(["shit", "piss", "fuck", "cunt", "cocksucker", "motherfucker", "tits"])

def carlin(text):
  if isinstance(text, float):
    print(text)
  
  splitty = text.split()
  ret = 0
  for x in splitty:
    if x in carlin_words:
      ret+= 1
  return ret

df['carlincount'] = df['comment_text'].apply(lambda x: carlin(x))
df['carlin'] = df['carlincount'].apply(lambda x: x > 0)

# See Carlin count
df.sort_values(by=['carlincount'], ascending=False)

"""We also predicted that reference to politics might be corelated to toxicity. Because the election, and in particular, Donald Trump, was such a prevalent topic during the time period that data was collected, Trump is mentioned in more then 100,000 comments."""

# Trump 
def containsTrump(text):
  if isinstance(text, float):
    print(text)
  
  splitty = text.split()
  for x in splitty:
    if x.lower() == "trump":
      return 1
  return 0

df['containsTrump'] = df['comment_text'].apply(lambda x: containsTrump(x))
print(len(df[df['containsTrump']==1]))

df.sort_values(by=['containsTrump','target'], ascending=False)

"""We also thought that the structure of sentences might be different between toxic and non-toxic comments. One way we decided to define that difference was to count the number of stop words in each comment to determine if this metric  correlates with either toxic or non-toxic comments.

"""

### Count stop words
from nltk.corpus import stopwords

stopdict = {}
isLoveReal = True
for x in stopwords.words('english'):
  stopdict[x] = isLoveReal

def stopCount(text):
  if isinstance(text, float):
    print(text)
  
  splitty = text.split()
  ret = 0
  for x in splitty:
    if x.lower() in stopdict:
      ret += 1
  return ret

df['stopCount'] = df['comment_text'].apply(lambda x: stopCount(x))

df.sort_values(by=['target'], ascending=True)

"""In addition to looking at stopwords, we looked at the number of sentences, average length of sentence, and certain punctuation usage to try to find potential overlaps. """

### SENTENCE LENGTH/COUNT
from nltk.tokenize import sent_tokenize
import numpy as np

def sentenceLength(text):
    replacey = text.replace('!','.')
    replacey = text.replace('?','.')
    splitty = replacey.split('.') # split on sentences
    length = len(splitty)
    cumsum = 0
    for x in splitty:
      cumsum += len(x.split()) # split on space
    if replacey[-1] == '.':
      length -=1
    return cumsum / length 

def sentenceCount(text):
    replacey = text.replace('!','.')
    replacey = text.replace('?','.')
    splitty = text.split('.')
    length = len(splitty)
    return length
    
df['sentenceLength'] = df["comment_text"].apply(lambda x : sentenceLength(x))

df['sentenceCount'] = df["comment_text"].apply(lambda x : sentenceCount(x))

## commma count

def commaCount(text):
  if isinstance(text, float):
    print(text)
  ret = 0
  for x in text:
    if x.lower() == ',':
      ret += 1
  return ret

df['commaCount'] = df["comment_text"].apply(lambda x : commaCount(x))

"""Here, we plotted sentence length and sentence count against the target value to get a better understanding of how these features are distributed across the dataset."""

df.plot.scatter('target', 'sentenceLength')

df.plot.scatter('target', 'sentenceCount')

"""Here we counted the number of times the 50 most common words in Toxic comments and 50 most common words in non toxic comments that we found in our Data exploration segment. for both lists we removed stopwords so that we would have less noise in the function"""

best_offensive_words_set = set(best_offensive_words.head(50).keys())
best_not_offensive_words_set = set(best_not_offensive_words.head(50).keys())

def calcbadWords(text):
  if isinstance(text, float):
    print(text)
  
  splitty = text.split()
  ret = 0
  for x in splitty:
    if x in best_offensive_words_set:
      ret+= 1
  return ret


def calcgoodWords(text):
  if isinstance(text, float):
    print(text)
  
  splitty = text.split()
  ret = 0
  for x in splitty:
    if x in best_not_offensive_words_set:
      ret+= 1
  return ret


df['calculatedBadcount'] = df['comment_text'].apply(lambda x: calcbadWords(x))

df['calculatedgoodcount'] = df['comment_text'].apply(lambda x: calcgoodWords(x))

"""Another feature we decided to explore was social justice buzzwords, which we obtained from Dalhousie University. We thought these words might be interesting to explore as they contain mentions of some identities may frequently be attacked in hate speech and other forms of offensive comments. Additionally, we thought that there was potential for these words to be used in a flame war scenerio in which two or more participants attack eachother.

https://en.wikipedia.org/wiki/Flaming_(Internet)#Flame_war
"""

#SOCIAL JUSTICE TERMS
buzzwords = {'Ableism','Accomplice','Ageism','Ally','Anti','Appropriation','Asexual','Assigned','Binary','Biphobia','Birth', 'Bisexual','Cisgender','Cissexism','Classism','Collusion','Coming','Competence','Conforming','Cultural','Discrimination', 'Empathy','Equality','Equity','Ethnocentrism','Expression','Gay','Gender','Genderqueer','Group','Hate','Heterosexism', 'Heterosexual','Homophobia','Identity','Inclusive','Internalized','Intersectionality','Intersex','Islamophobia','Jewish', 'Justice','Lesbian','Neutral','Non','Oppression','Orientation','Out','Pansexual','Power','Prejudice','Privilege','Profiling', 'Pronoun','Queer','Racial','Racism','Religious','Semitism','Sex','Sexism','Sexual','Social','Stereotype','Transgender', 'Transphobia','Xenophobia', 'Trump'}

def buzzWords(text):
  if isinstance(text, float):
    print(text)
  
  splitty = text.split()
  ret = 0
  for x in splitty:
    if x in buzzwords:
      ret+= 1
  return ret

df['buzzcount'] = df['comment_text'].apply(lambda x: buzzWords(x))

#taken/modified from https://www.dal.ca/dept/hres/education-campaigns/educational-resources/definitions.html

"""At the suggestion of Dr. Ives, we opted to calculate the supposed reading level of comments and add them as a feature. We opted for the Automated readability index since it relies on statistics that are easily calcuated by parsing the text automatically. The index attempts to approximate the US grade level of given text.


https://en.wikipedia.org/wiki/Automated_readability_index

https://www.tutorialspoint.com/readability-index-in-python-nlp

"""

##NLP reading level

def wordCount(text):
    replacey = text.replace('!',' ')
    replacey = text.replace('?',' ')
    replacey = text.replace('-',' ')
    replacey = text.replace('.',' ')
    splitty = replacey.split(' ') 
    
    setty = set()
    for x in splitty:
      setty.add(x) 
    
    return len(setty)

def charPerWord(text):
    replacey = text.replace('!',' ')
    replacey = text.replace('?',' ')
    replacey = text.replace('-',' ')
    replacey = text.replace('.',' ')
    splitty = replacey.split(' ') 
    
    listy = []
    cumsum = 0
    for x in splitty:
      listy.append(x) 
      cumsum+= len(x)
    return cumsum / len(listy)

df['wordCount'] = df["comment_text"].apply(lambda x : wordCount(x))

df['charPerWord'] = df["comment_text"].apply(lambda x : charPerWord(x))

df['AutomatedReadabilityIndex'] = 4.71 * df['charPerWord'] + 0.5 * df['sentenceLength'] -21.43

df.sort_values(by=['target'], ascending=True)

"""ARI is supposed to gadge the grade level of text but because of the weight placed on longer length words, this can give spam a abnormally high value. Fortunately this adds value to the model in the form of predictive power. Comments with abnormally high ARI will skew more toxic."""

df.sort_values(by=['AutomatedReadabilityIndex'], ascending=True)

"""Here we plot the ARI vs the target """

import matplotlib as plt
df.plot.scatter('target', 'AutomatedReadabilityIndex')

valid_dfdf_filtered = df[df['AutomatedReadabilityIndex'] <= 300]

valid_dfdf_filtered.plot.scatter('AutomatedReadabilityIndex','target')

df.plot.scatter('target', 'carlincount')

"""In addition to the words that we calculated from data exploration, we also counted the occurrences of common english swear words in the comment text"""

# Taken/modified from wikitionary: https://en.wiktionary.org/wiki/Category:English_swear_words
badWordsSet = set(['arse', 'ass', 'asshole', 'bastard', 'bitch', 'bollocks', 'brotherfucker', 'bugger', 'bullshit', 'childfucker', 'cocksucker', 'crap', 'cunt', 'damn', 'effing', 'fatherfucker', 'frigger', 'fuck', 'goddamn', 'godsdamn', 'hell', 'holy', 'shit', 'horseshit', 'Jesus', 'Christ', 'motherfucker', 'nigga', 'piss', 'prick', 'shit', 'shitass', 'sisterfucker', 'slut', 'bitch', 'whore', 'Jesus', 'twat'])

def badWords(text):
  if isinstance(text, float):
    print(text)
  
  splitty = text.split()
  ret = 0
  for x in splitty:
    if x in badWordsSet:
      ret+= 1
  return ret

df['badcount'] = df['comment_text'].apply(lambda x: badWords(x))

df_small_not_offensive_bad_count = df.loc[df['label'] == 1]
df_small_offensive_bad_count = df.loc[df['label'] == 0]

"""Below is the distribution of bad word counts between offensive comment and non offensive comments. The distribution is heavily right skewed mainly due to the underlying data. Many comments are short to begin with therefore they will have few bad words regardless of their offensiveness. While small, there is a slightly longer tail of bad word counts for offensive comments versus not offensive. """

# Histogram frequency of bad word count
plt.hist(df_small_offensive_bad_count['badcount'], log=True)

# Histogram frequency of bad word count
plt.hist(df_small_not_offensive_bad_count['badcount'], log=True)

"""Here we have a correlation matrix between all of our features and the labels. As you can see our calculated good, Calculated bad, Buzzwords and the Trump features were most correlated with our labels."""

# TODO correlation matrix
import seaborn as sns
import matplotlib.pyplot as plt
corr = df.corr()

#### THIS IS TAKEN FROM SEABORN DOCS https://seaborn.pydata.org/examples/many_pairwise_correlations.html
# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

##### TAKEN VERBITAM ^^^^^^^^

"""### Logistic Regression

The first model we used to train on these features was this logistic model. It was initially very 'accurate' but had a low AUC. We realized that it was only achieving high accuracy because it was guessing that the vast majority of comments were non toxic. To deal with this issue we changed the class weight to balanced and acheived a higher AUC at the cost of accuracy. Ultimately the model was not very good but did better then the pre trained roBERTa model that we use later in the notebook
"""

####### Logistic Modeling

'''looked at some sample code from https://towardsdatascience.com/linear-regression-in-python-sklearn-vs-excel-6790187dc9ca'''

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import r2_score
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

#evenDF = df.sort_values(by=['label'], ascending=False).head(288668) # this was an dataframe with  even split between classes. We thought it would help the model but it did not

y = df['label']

x = df.drop(columns=['target','label','comment_text','id'])


sc_X = StandardScaler() # SCALING GOES HERE
x = sc_X.fit_transform(x)

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)

reg = LogisticRegression(class_weight='balanced',max_iter = 400).fit(X_train, y_train)
print("The Logistic regression score on training data is ", round(reg.score(X_train, y_train),2))

predict=reg.predict(X_test)

labels =  list(y_test)
eval = [x for i, x in enumerate(predict) if predict[i]== labels[i]]

from sklearn.metrics import roc_auc_score
print(roc_auc_score(list(y_test), predict))

print(len(eval)/len(predict))### confusion matrix
print(len([x for x in predict if x == 0]))
print(len([x for x in predict if x == 1]))
print(len([x for x in labels if x ==0 ]))
print(len([x for x in labels if x ==1 ]))

"""We also tried putting the features we generated into a random forest. This had the same problems as the logistic model although it generally did a bit better. """

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

clf = RandomForestClassifier(class_weight='balanced',max_depth=2, random_state=0)
clf.fit(X_train, y_train)

print(clf.score(X_train, y_train))

predict = clf.predict(X_test)

labels =  list(y_test)
eval = [x for i, x in enumerate(predict) if predict[i]== labels[i]]


print(roc_auc_score(y_test, predict))
print(clf.score(X_test, y_test))

print(clf.score(X_test, y_test))
print(len([x for x in predict if x == 0]))
print(len([x for x in predict if x == 1]))
print(len([x for x in labels if x ==0 ]))
print(len([x for x in labels if x ==1 ]))

"""## Pretrained RoBERTa

We utilize an advanced Bidirectional Encoder Representations from Transformers (BERT) model pretrained on offensive twitter comments to predict the offensiveness of hitherto unseen comments. Instead of a traditional BERT model, we leverage RoBERTa, which in contrast to BERT, dynamically masks outputs. As defined by the model’s creators: “BERT relies on randomly masking and predicting tokens. The original BERT implementation performed masking once during data preprocessing, resulting in a single static mask. To avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training. Thus, each training sequence was seen with the same mask four times during training.
We compare this strategy with dynamic masking where we generate the masking pattern every time we feed a sequence to the model. This becomes crucial when pretraining for more steps or with larger datasets.” As such, RoBERTa performs significantly better than its contemporary models due to its innovative masking paradigm.
"""

pip install transformers numpy scipy

from transformers import AutoModelForSequenceClassification
from transformers import TFAutoModelForSequenceClassification
from transformers import AutoTokenizer
import numpy as np
from scipy.special import softmax
import csv
import urllib.request

MODEL = f"cardiffnlp/twitter-roberta-base-offensive"

tokenizer = AutoTokenizer.from_pretrained(MODEL)

model = AutoModelForSequenceClassification.from_pretrained(MODEL)
model.save_pretrained(MODEL)

"""Test the model with known offensive and non-offensive comments"""

text = "this is a very cute test of my stuff"
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
scores = output[0][0].detach().numpy()
scores = softmax(scores)

scores

text = "i hate you bitch"
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
scores = output[0][0].detach().numpy()
scores = softmax(scores)

scores

text = "I love you sooo much"
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
scores = output[0][0].detach().numpy()
scores = softmax(scores)

scores

new_X_test = list(df["comment_text"])
new_y_test = list(df["label"])

"""Downsample then test and evaluate"""

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(new_X_test, new_y_test, test_size=0.00005, random_state=1)
print(len(X_test_1))

pred_y = []
for example in X_test_1:
  encoded_input = tokenizer(example, return_tensors='pt')
  output = model(**encoded_input)
  scores = output[0][0].detach().numpy()
  scores = softmax(scores)
  pred_y.append(False if scores[0] >= .5 else True)

from sklearn.metrics import accuracy_score

accuracy_score(y_test_1, pred_y)

roc_auc_score(y_test_1, pred_y)

"""## RoBERTa: fine tuning pretrained model with toxicity data

In addition to using the pretrained RoBERTa model above, we decided to try tuning a base RoBERTa model using our own toxicity data. Due to processor and time constraints, we were only able to run the models on our smallest sampled dataset (1/100th of the full dataset). Even with the smallest dataset, we received an AUC of ~0.86. From conversations with Dr. Ives and our own research, we have reason to believe that our RoBERTa model would perform even better if we were able to run our full dataset, which we are unable to do at the moment due to Colab's GPU limits. 

"""

# Roberta and Bert models based on examples found at: https://huggingface.co/docs/transformers/model_doc/roberta
# Building tensorflow datasets modeled after: https://www.tensorflow.org/guide/data

!pip install -q transformers==4.6.1 tensorflow==2.4.1

from transformers import RobertaTokenizer, TFRobertaModel
import tensorflow as tf
import pandas as pd
import numpy as np

tf.random.set_seed(1)

# Loading tokenizer/model
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
roberta = TFRobertaModel.from_pretrained('roberta-base')

"""To help adjust for the fact that our dataset contains an unbalanced ratio of non-toxic to toxic comments, we calculated this ratio to be used as a class weight in the BERT and RoBERTa models. """

# Determine average sentence length to get an estimate for MAX_LEN
df["length"] = df["comment_text"].apply(lambda x : len(x))
print(df.length.describe())

# Determine toxicity ratio of data for class weights
counts = dict(df['label'].value_counts())
print(counts)
class_weight = {0:1, 1:(counts[0] / counts[1])}

# Parameters
epoch = 1
ngpus = 1
batch_size = 16*ngpus
MAX_LEN = 256 # max token length

# Tokenize sentences into fixed size token ID vectors and corresponding labels
def encode_text(txt, labels, tokenizer):
    for tt, ll in zip(txt, labels):
        encoded = tokenizer.__call__(tt, max_length = MAX_LEN, add_special_tokens = True, truncation = True,
                                     padding = 'max_length', return_attention_mask = True, return_tensors='tf')

        yield (encoded.input_ids[0], encoded.attention_mask[0], ll)

# Convert training data into Tensorflow dataset
df_train_used = df_train_sm
ds = tf.data.Dataset.from_generator(lambda: encode_text(df_train_used['comment_text'], df_train_used['label'], tokenizer), 
                                    output_types = (tf.int32, tf.int32, tf.float64),
                                    output_shapes = ((MAX_LEN,),(MAX_LEN,),()))
ds = ds.map(lambda a,b,c: ({"input_ids":a, "attention_mask":b}, c)).batch(batch_size).repeat(epoch)

# Convert test data
df_test_used = df_test_sm
ds_test = tf.data.Dataset.from_generator(lambda: encode_text(df_test_used['comment_text'], df_test_used['label'], tokenizer), 
                                    output_types = (tf.int32, tf.int32, tf.float64),
                                    output_shapes = ((MAX_LEN,),(MAX_LEN,),()))
ds_test = ds_test.map(lambda a,b,c: ({"input_ids":a, "attention_mask":b}, c)).batch(batch_size)

# Set up model
class text_model(tf.keras.Model):
    def __init__(self, base):
        super(text_model,self).__init__(name="text_model")
        self.bert = base
        self.dense3 = tf.keras.layers.Dense(1, activation = 'sigmoid', name = "dense3") 

    def call(self, x):
        yb = self.bert(x).pooler_output
        yb = self.dense3(yb)
        return yb

# Use GPU if available
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  # Run without GPU if device not found
  n = df_train_used.shape[0]
  optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5)
  loss_fn = 'binary_crossentropy'  
  model = text_model(roberta)
  model.compile(loss=loss_fn, optimizer=optimizer, metrics=[tf.keras.metrics.AUC()])
  model.fit(ds, batch_size = batch_size, steps_per_epoch = n/batch_size/ngpus,
            epochs = epoch, validation_data = ds_test, class_weight = class_weight)
  model.save_weights("/content/model.weights")
else:
  with tf.device('/device:GPU:0'):
    n = df_train_sm.shape[0]
    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5)
    loss_fn = 'binary_crossentropy' 
    model = text_model(roberta)
    model.compile(loss=loss_fn, optimizer=optimizer, metrics=[tf.keras.metrics.AUC()])
    model.fit(ds, batch_size = batch_size, steps_per_epoch = n/batch_size/ngpus,
              epochs = epoch, validation_data = ds_test, class_weight = class_weight)
    model.save_weights("/content/model.weights")

    import time
    t1 = time.time()
    p_roberta = model.predict(ds_test)[:,0]
    t2 = time.time()
    print("total prediction time = ", t2-t1)

from sklearn.metrics import roc_auc_score
roc_auc_score(df_test_sm['label'], p_roberta)

"""## BERT
After running RoBERTa, we decided to try BERT to see if it would be better suited for our data. Instead of using BERT to vectorize our data as we did with Word2vec,  we connected the BERT output to a sigmoid unit and tried to minimize the loss (binary cross entropy). In this process, known as fine-tuning, we adjust the weights of the BERT model to better fit our labels. 

Due to GPU constraints, we ran this on our smallest sampled dataset (1/100th of the original) and were able to achieve an AUC of ~0.86. On different runs, we were able to achieve AUC up to 0.92. We predict that we would achieve an even higher AUC if we were able to train with our full dataset. 
"""

# Roberta and Bert models based on examples found at: https://huggingface.co/docs/transformers/model_doc/roberta
# Building tensorflow datasets modeled after: https://www.tensorflow.org/guide/data

# Import BERT and set seed
from transformers import BertTokenizer, TFBertModel
tf.random.set_seed(1)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert = TFBertModel.from_pretrained('bert-base-uncased')

# Parameters
epoch=1
ngpus = 1
batch_size = 16*ngpus
MAX_LEN = 256 # max token length

# Tokenize sentences into fixed size token ID vectors and corresponding labels
def encode_text(txt, labels, tokenizer):
    for tt, ll in zip(txt, labels):
        encoded = tokenizer.__call__(tt.lower(), max_length = MAX_LEN, add_special_tokens = True, truncation = True,
                                     padding = 'max_length', return_attention_mask = True, return_tensors='tf')
        yield (encoded.input_ids[0], encoded.attention_mask[0], ll)

# Convert training data into Tensorflow dataset
df_train_used = df_train_sm
ds = tf.data.Dataset.from_generator(lambda: encode_text(df_train_used['comment_text'], df_train_used['label'], tokenizer), 
                                    output_types = (tf.int32, tf.int32, tf.float64),
                                    output_shapes = ((MAX_LEN,),(MAX_LEN,),()))
ds = ds.map(lambda a,b,c: ({"input_ids":a, "attention_mask":b}, c)).batch(batch_size).repeat(epoch)

# Convert test data
df_test_used = df_test_sm
ds_test = tf.data.Dataset.from_generator(lambda: encode_text(df_test_used['comment_text'], df_test_used['label'], tokenizer), 
                                    output_types = (tf.int32, tf.int32, tf.float64),
                                    output_shapes = ((MAX_LEN,),(MAX_LEN,),()))
ds_test = ds_test.map(lambda a,b,c: ({"input_ids":a, "attention_mask":b}, c)).batch(batch_size)

# Set up model
class text_model(tf.keras.Model):
    def __init__(self, base):
        super(text_model,self).__init__(name="text_model")
        self.bert = base
        self.dense3 = tf.keras.layers.Dense(1, activation = 'sigmoid', name = "dense3") 

    def call(self, x):
        yb = self.bert(x).pooler_output
        yb = self.dense3(yb)
        return yb

# Use GPU if available
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  # Run without GPU if device not found
  n = df_train_used.shape[0]
  optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5)
  loss_fn = 'binary_crossentropy'  
  model = text_model(bert)
  model.compile(loss=loss_fn, optimizer=optimizer, metrics=[tf.keras.metrics.AUC()])
  model.fit(ds, batch_size = batch_size, steps_per_epoch = n/batch_size/ngpus,
            epochs = epoch, validation_data = ds_test, class_weight = class_weight)
  model.save_weights("/content/model.weights")
else:
  with tf.device('/device:GPU:0'):
    n = df_train_used.shape[0]
    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5)
    loss_fn = 'binary_crossentropy' 
    model = text_model(bert)
    model.compile(loss=loss_fn, optimizer=optimizer, metrics=[tf.keras.metrics.AUC()])
    model.fit(ds, batch_size = batch_size, steps_per_epoch = n/batch_size/ngpus,
              epochs = epoch, validation_data = ds_test, class_weight = class_weight)
    model.save_weights("/content/model.weights")

    import time
    t1 = time.time()
    p_bert = model.predict(ds_test)[:,0]
    t2 = time.time()
    print("total prediction time = ", t2-t1)

roc_auc_score(df_test_used['label'], p_bert)

"""#Obstacles

One of the obstacles we faced included successfully learning the syntax of at least five different modeling packages. 

We also faced difficulties determining relevant features to extract. Sentiment analysis is a complex problem to solve and there is no singular best accepted approach.

Perhaps the biggest obstacle that we faced in our project was the major class imbalance between toxic and non-toxic data. We addressed this by using a lower toxicity threshold to generate our binary labels and also by addressing this directly in our models through the use of class weights.

# Conclusion

In conclusion, our most successful models include logistic regression on tf-idf and our fine-tuned roBERTa and BERT models, which is a bit of surprise considering that the former is a much simpler model. All performed similarly (with an AUC around 0.84 - 0.86), which serves as a lesson to not discount simple models. However, we believe that the BERT has more potential. 

In the future, given more GPU resources, we hope to be able to fine-tune BERT with the full dataset. Additional avenues to explore in the future include analysis based on stemming to catch offensive words that share the same root, ensemble models, higher data fidelity, better data labeling, more feature engineering heuristics. Utilizing other pretrained models may also be a fruitful pursuit. The sentence structure such as separating the subject and predicate could provide useful context clues when predicting toxicity. Performing Named Entity Recognition could also add more signal to the model’s weights.
"""